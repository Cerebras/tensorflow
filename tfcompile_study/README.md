## Cerebras Systems use case for tfcompile
This document evaluates `tfcompile` for Cerebras’ use cases.

 Cerebras Systems is building a new chip and system to accelerate deep learning workloads. To support TensorFlow, we use XLA as the input to our stack. Currently, to get the XLA graph, we use an XLA extraction tool developed [in house](https://github.com/Cerebras/tensorflow/tree/vishal/tf14_hlopass/tensorflow/tools/xla_extract).

 In spirit, `tfcompile` is exactly what we need. However, the following concerns currently prevent us from being able to use `tfcompile` as a viable solution:
  1. It is impractical to have a user manually generate a config file for every model they want to run through the tool, especially for large models.
     * One way to significantly improve this workflow is to automatically generate the config file. All the information needed is already provided in the graph.pbtxt.<br/>
     * We have included an example script that generates the config file for our provided examples in utils.py (for r1.14). Ideally, a solution like this can be adopted for `tfcompile`.
  1. Naming of variables is prohibitively restrictive.<br/>
     * Currently, naming must follow C++ naming conventions, which does not support special characters such as `/`. This means `tfcompile` throws an error any time a user uses variable_scope or certain TF layers like dropout, which will automatically include `/` in the tensor name.<br/>
     * Supporting default TensorFlow naming conventions would fix this.<br/>
     * In our branch, we worked around this for most use cases by simply special casing support for `/` in the codegen.cc (Refer to ‘TensorFlow Version’ below).<br/>
  1. RNNs are not supported through `tfcompile`, even though XLA in 1.13+ does.<br/>
     *  `tf.while_loop` throws an `Invalid Arguments` error that does not correctly use the `maximum_iterations` parameter, even if it has been specified: <br/>
     ```
     INVALID ARGUMENTS: XLA compilation requires a fixed stack size upper bound. If you are using tf.while_loop, set the
     maximum_iterations parameter to fix this issue.
     ```
     * More details in ‘Experiments’ below.<br/>

Addressing these concerns would make `tfcompile` an ideal XLA extraction solution for 3rd party hardware vendors using XLA as input to their systems.

In our evaluation of `tfcompile`, we ran 15 tests with different common neural network components through the tool. In each case, the config was automatically generated by the script provided, and we verified the generated XLA using the environment variable `TF_CPP_MIN_VLOG_LEVEL=3`. We have documented all errors we encountered below.

**Experiments:**
1. Fully connected layers  using `GradientDescentOptimizer` - (`examples/fc`)
2. Fully connected layers using `AdamOptimizer` - (`examples/fc_adam`)
3. Fully connected layers with `batch_normalization(training=True/False)`, using `GradientDescentOptimizer` - (`examples/fc_batchnorm`)
4. Fully connected layers with `batch_normalization(training=True)`, using `AdamOptimizer` - (`examples/fc_batchnorm_adam`)
5. Fully connected layers with `keras.BatchNormalization(training=True/False)`, using `GradientDescentOptimizer` - (`examples/fc_kerasbatchnorm`)
6. CNN + fully connected layers using `GradientDescentOptimizer` - (`examples/conv_fc`)
7. CNN + fully connected layers using `AdamOptimizer` - (`examples/conv_fc_adam`)
8. CNN + fully connected layers with `batch_normalization(training=True/False)`, using `GradientDescentOptimizer` - (`examples/conv_fc_batchnorm`)
9. CNN + fully connected layers with `batch_normalization(training=True)`, using `AdamOptimizer` - (`examples/conv_fc_batchnorm_adam`)
10. Using `BasicRNNCell` with `dynamic_rnn` - (`examples/dynamic_rnn`)
11. Using `tf.keras.layers.SimpleRNNCell` and `tf.keras.layers.RNN` - (`examples/keras_rnn`)
12. Using `BasicRNNCell` with `dynamic_rnn` with seq_length specified - (`examples/rnn_seq_length`)

**Experiments which failed:**
 1. Using `BasicRNNCell` with `dynamic_rnn` Error:
    ```Bash
    INVALID ARGUMENTS: XLA compilation requires a fixed stack size upper bound. If you are using tf.while_loop, set the
    maximum_iterations parameter to fix this issue.
    [[{{node dynamic_rnn/gradients/dynamic_rnn/func/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3/f_acc}}]]
    ```
 2. Using `tf.keras.layers.SimpleRNNCell` and `tf.keras.layers.RNN` Error: same error as above.
 3. Using `BasicRNNCell` with `dynamic_rnn` with seq_length specified Error: same error as above.
   * Our sample extraction tool is able to extract XLA successfully for all 3 of the above experiments.
   * While developing our extraction tool, we ran into a similar issue - in the process of translating an RNN model to XLA, TensorFlow generated multiple function defs. Our understanding was that one function def is for the xla_cpu device, and another was for the xla_cpu_jit device. We saw the same error as above when using the xla_cpu_jit device-specific version of the function def. Since `tfcompile` also uses xla_cpu_jit as the default device, it may be a similar issue. 

**To replicate the experiments:**
  1. **TensorFlow Version:**  
     * Based off of the TensorFlow r1.14 branch.
     * Branch used can be found [here](https://github.com/Cerebras/tensorflow/tree/vishal/tf14_tfcompile/).
     * This tf branch was built using the default settings in `./configure`.
     * The only change we made to the code base is in `tensorflow/compiler/aot/codegen.cc#L774` to support `/` in node names.
       * Originally, it supports names that follow the C++11 Standard naming convention, so this change is to handle variable_scope.

  2. **Inputs given to tfcompile:**  
     * `tfcompile` command line tool requires at least:
       1. `--graph` (.pbtxt or .pb)
         * graph is extracted from the model (setting `add_shapes=True`).
       2. `--config` (.pbtxt or .pb)
         * This contains `feed` (input) nodes, `fetch` (output) nodes and `variable` nodes.
         * The examples in the TensorFlow repository, are all generated manually, but for those examples and the ones we tested, we generated the config file based of the graph.
         * the config is based on the proto file (`tensorflow/compiler/tf2xla/tf2xla.proto`)
       3. `--cpp_class` (for generated .h  and .o files)
         * cpp_class is set to the same as in their examples (`mynamespace::MyComputation`).

  3. **To compile tf2xla.proto**  
     ```Bash
     protoc --python_out=/path_to_store_compiled_file --proto_path=/path_to_tensorflow_dir/tensorflow   tensorflow/compiler/tf2xla/tf2xla.proto
     ```
     * Currently stored [here](https://github.com/Cerebras/tensorflow/blob/vishal/tf14_tfcompile/tfcompile_study/tf2xla_pb2.py).

  4. **To run tfcompile:**  
     ```Bash
     TF_CPP_MIN_VLOG_LEVEL=3 /path_to_tensorflow/tensorflow/bazel-bin/tensorflow/compiler/aot/tfcompile --graph=graph_model_fn.pbtxt --config=config_model_fn.config.pbtxt --cpp_class="mynamespace::MyComputation"
     ```
     * Is being called within the `run` function through a subprocess (https://github.com/Cerebras/tensorflow/blob/vishal/tf14_tfcompile/tfcompile_study/utils.py#L75).


  5. **Files to replicate our experiments:**  
     1. utils.py - contains config generation script and run script to generate the `graph_def` and the config script from the `model_fn` and the `input_fn`.
     2. tf2xla_pb2.py - compiled protobuf file of `tf2xla.proto` for python import.
     3. experiments - directory with each subdirectory containing a specific model.
        1. each experiment is a pytest.
